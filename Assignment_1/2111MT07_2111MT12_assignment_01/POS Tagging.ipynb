{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52df65fc",
   "metadata": {},
   "source": [
    "importing json module and reading data as well as checking type, format of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff50e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words is: 81147\n",
      "Most used tag is:  NN\n",
      "It is used for 12721 times \n",
      "total number of tags in dataset is:  41\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import time\n",
    "import random\n",
    "tag_count={}\n",
    "#defining dictionary for tag count\n",
    "now=time()\n",
    "f = open('penn-data.json')\n",
    "data = json.load(f)\n",
    "#print(data)\n",
    "random.shuffle(data)\n",
    "#adding each tag top dictionary\n",
    "for point in data:\n",
    "    tags=point[1]\n",
    "    for tag in tags:\n",
    "        if tag not in tag_count:\n",
    "            tag_count[tag]=1\n",
    "        else:\n",
    "            tag_count[tag]+=1\n",
    "freq=0\n",
    "most_used_tag=''\n",
    "total=0\n",
    "#getting most used tag for case of missing word\n",
    "for tag,count in tag_count.items():\n",
    "    total+=count\n",
    "    if count>freq:\n",
    "        freq=count\n",
    "        most_used_tag=tag\n",
    "        \n",
    "print('Total number of words is:',total )        \n",
    "print('Most used tag is: ',most_used_tag)\n",
    "print(f'It is used for {freq} times ')\n",
    "print(\"total number of tags in dataset is: \",len(tag_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc2b7d",
   "metadata": {},
   "source": [
    "splitting data into train and test in ration of 80:20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26209d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples:  3914\n",
      "No. of training examples:  3131\n",
      "No. of testing examples:  783\n"
     ]
    }
   ],
   "source": [
    "train_nos=round(0.8*len(data))#total number of training examples  \n",
    "print('Total number of examples: ',len(data))\n",
    "print('No. of training examples: ',train_nos)\n",
    "test_nos=len(data)-train_nos#total number of test examples\n",
    "print('No. of testing examples: ',test_nos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341405b",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "1. Removing special characters such as comms, inverted commas, full-stops.\n",
    "2. Lowering each character so that same word cannot oocupy different serial numbers which are being assigned next step.\n",
    "3. Adding start and end tag at begining and ending of each sequence and corrosponding tags.\n",
    "Above pre processed sequences and tag lists are written in seq.txt and tag.txt.\n",
    "\n",
    "Tokenization\n",
    "Preparing dictionary of words in test data set for better access. Assigning unique serial number to each word.(word_dict)\n",
    "Similar dictionary for tags is prepared in above cells.(tag_dict)\n",
    "\n",
    "Tokenization will also give total number of unique words, tags which can be used in later stages if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7fde9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'<start>':0}\n",
    "tag_dict={'<start>':0}\n",
    "word_count=1\n",
    "tag_count=1\n",
    "seq_file = open(\"seq.txt\",\"w\")\n",
    "tag_file=open(\"tag.txt\",\"w\")\n",
    "for tag_seq in data:\n",
    "    seq=tag_seq[0].split(' ')\n",
    "    tag=tag_seq[1]\n",
    "    for i in range(len(seq)):\n",
    "        seq[i]=seq[i].lower()\n",
    "        if \"'\" in seq[i]:\n",
    "            seq[i]=seq[i].replace(\"'\",'',100)\n",
    "        if '.' in seq[i]:\n",
    "            seq[i]=seq[i].replace('.','',100)\n",
    "        if ',' in seq[i]:\n",
    "            seq[i]=seq[i].replace(',','',100)\n",
    "        if ',' in seq[i]:\n",
    "            seq[i]=seq[i].replace('\"','',100)\n",
    "        if '``' in seq[i]:\n",
    "            seq[i]=seq[i].replace('``','',100)\n",
    "        if '?' in seq[i]:\n",
    "            seq[i]=seq[i].replace('``','',100)\n",
    "        if seq[i]=='&':\n",
    "            seq[i]='and'\n",
    "            \n",
    "            \n",
    "    seq=['<start>']+seq+['<end>']\n",
    "    out_seq=' '\n",
    "    out_tag=' '\n",
    "    out_seq=out_seq.join(seq)\n",
    "    \n",
    "    tag=['<start>']+tag+['<end>']\n",
    "    out_tag=out_tag.join(tag)\n",
    "    seq_file.write(out_seq)\n",
    "    seq_file.write('\\n')\n",
    "    tag_file.write(out_tag)\n",
    "    tag_file.write('\\n')\n",
    "\n",
    "\n",
    "seq_file.close()\n",
    "tag_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd133f",
   "metadata": {},
   "source": [
    "Preparing test dataset. \n",
    "Now we have tags and sequences in required format. \n",
    "That data is splitted in below step. It is being stored seq_train.txt amnd tag_train.txt files.\n",
    "during writing tags and sequences they are being written using tokens in word_dict and tag_dict so that we can easily use them in later stages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce59b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_file=open(\"tag.txt\",\"r\")\n",
    "seq_file = open(\"seq.txt\",\"r\")\n",
    "train_seq=open(\"seq_train.txt\",\"w\")\n",
    "train_tag=open(\"tag_train.txt\",\"w\")\n",
    "seq_file.seek(0)\n",
    "tag_file.seek(0)\n",
    "word_dict={'<start>':0}\n",
    "tag_dict={'<start>':0}\n",
    "word_count=1\n",
    "tag_count=1\n",
    "for i in range(train_nos):\n",
    "    temp=seq_file.readline().split(' ')\n",
    "    out=' '\n",
    "    for word in temp:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word]=word_count\n",
    "            word_count+=1\n",
    "    x=[str(word_dict[word]) for word in temp]\n",
    "    out=out.join(x)\n",
    "    train_seq.write(out)\n",
    "    train_seq.write('\\n')\n",
    "    temp=tag_file.readline().split(' ')\n",
    "    out=' '\n",
    "    for tag in temp:\n",
    "        if tag not in tag_dict:\n",
    "            tag_dict[tag]=tag_count\n",
    "            tag_count+=1\n",
    "    x=[str(tag_dict[tag]) for tag in temp]\n",
    "    out=out.join(x)\n",
    "    train_tag.write(out)\n",
    "    train_tag.write('\\n')\n",
    "test_tag=open('test_tag.txt','w')\n",
    "test_seq=open('test_seq.txt','w')\n",
    "for j in range(train_nos,len(data)):\n",
    "    temp_seq=seq_file.readline()\n",
    "    temp_tag=tag_file.readline()\n",
    "    test_tag.write(temp_tag)\n",
    "    #test_tag.write('\\n')\n",
    "    test_seq.write(temp_seq)\n",
    "    #test_seq.write('\\n')\n",
    "train_tag.close()\n",
    "train_seq.close()\n",
    "tag_file.close()\n",
    "seq_file.close()\n",
    "test_tag.close()\n",
    "test_seq.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeac9c7",
   "metadata": {},
   "source": [
    "algorithm may encounter a case where some specific tag is present in test dataset but not in train dataset(foreign tag). For that case all tags must be considered. Though transisition and initial probabilities will be zero for corrosponding tag but they must be there to ensure uninterupted working of algorithm. \n",
    "\n",
    "For a case of foereign word we are considering most used for that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d972308",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_file=open('tag.txt','r')\n",
    "for i in range(len(data)):\n",
    "    temp=tag_file.readline().split(' ')\n",
    "    for tag in temp:\n",
    "            if tag not in tag_dict:\n",
    "                tag_dict[tag]=tag_count\n",
    "                tag_count+=1\n",
    "tag_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afb7b0",
   "metadata": {},
   "source": [
    "preparing emission probability matrix and transisition probability matrix. \n",
    "Dimensions for emission probability matrix: word_count x tag_count\n",
    "dimension for transisition probability matrix: tag_count x tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cf8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emit=np.zeros([word_count,tag_count])\n",
    "trans=np.zeros([tag_count,tag_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83654afe",
   "metadata": {},
   "source": [
    "calculating both matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd4084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f60bf03b76b6>:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  trans=trans/trans.sum(axis=1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "def matrix_calculator(emit,trans,train_nos):\n",
    "    train_seq=open('seq_train.txt','r')\n",
    "    train_tag=open('tag_train.txt','r')\n",
    "    train_seq.seek(0)\n",
    "    train_tag.seek(0)\n",
    "    for i in range(train_nos):\n",
    "        temp_seq=train_seq.readline()\n",
    "        temp_tag=train_tag.readline()\n",
    "        input_seq=[int(w) for w in temp_seq.split(' ')]\n",
    "        #parasing each sequence in train sequence \n",
    "        input_tag=[int(t) for t in temp_tag.split(' ')]\n",
    "        #arsing each tag in train tags\n",
    "        for i in range(len(input_seq)):\n",
    "            emit[input_seq[i]][input_tag[i]]+=1\n",
    "            #incrementing corrosponding count of emission probability matrix\n",
    "        for j in range(len(input_tag)-1):\n",
    "            trans[input_tag[j]][input_tag[j+1]]+=1\n",
    "            #incrementing corrosponding count of transmission probability matrix\n",
    "\n",
    "    train_seq.close()\n",
    "    train_tag.close()\n",
    "    emit=emit/emit.sum(axis=0, keepdims=True)\n",
    "    #dividing each element by sum of elements in corrosdponding column\n",
    "    trans=trans/trans.sum(axis=1, keepdims=True)\n",
    "    #dividing each element by sum of elements in corrosdponding row\n",
    "    return emit,trans\n",
    "train_nos=round(0.8*len(data))\n",
    "emit=np.zeros([word_count,tag_count])\n",
    "trans=np.zeros([tag_count,tag_count])\n",
    "emit,trans=matrix_calculator(emit,trans,train_nos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b812a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00410958904109589"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying matrix \n",
    "trans[tag_dict['MD']][tag_dict['DT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713c668",
   "metadata": {},
   "source": [
    "Now that we have found transition and emission matrix we will proceed toward testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0aee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing words is:  1368\n"
     ]
    }
   ],
   "source": [
    "#getting data for number of missing words\n",
    "test_seq=open(\"test_seq.txt\",'r')\n",
    "test_seq.seek(0)\n",
    "missing=0\n",
    "for line in test_seq:\n",
    "    for word in line.split(' '):\n",
    "        if word not in word_dict:\n",
    "            missing+=1\n",
    "print('Number of missing words is: ',missing)\n",
    "test_seq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af0efef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defing sdtart tag num and end tag num\n",
    "start_tag_num=word_dict['<start>']\n",
    "end_tag_num=tag_dict['<end>\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd833323",
   "metadata": {},
   "source": [
    "For testing we have used OOP approach. \n",
    "Each node (node class) is object with attributes:\n",
    "1. tag_num: tag number for corrosponding time step\n",
    "2. word: word at that time step\n",
    "3. emit_prob: emission probability of word and that tag\n",
    "4. parent: best possible node tag number in previous tag(used in bigram model)\n",
    "5. grandparent: best possible node tag number based on previous two tags\n",
    "6. grandparent_prob: transistion probability * emission probability based on previous two nodes\n",
    "7. grandparent_node: best possible node based on previous two timesteps\n",
    "in node object we have defined a step for foreign word. In that step if word is foreign word then node with most used tag is having emnission probability one else it will be zero.\n",
    "\n",
    "each time step (class time_instance) is object with attributes:\n",
    "1. word=word at at that timestep\n",
    "2. tag_count=total number of nodes at that particular timesteps\n",
    "3. nodes=it is list ciontating all node objects at that particular timestep\n",
    "\n",
    "timeline object\n",
    "It is basially our whole structure through which our path will move and output is obtained using viterbi algorithm. \n",
    "It is having following attributes:\n",
    "1. seq=Input sequence for which tag sequence is to be determined\n",
    "2. tag_count=total number of tags in whole dataset \n",
    "3. timesteps=timestep object for each word created using above class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e01320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class node:\n",
    "    out=[]\n",
    "    def __init__(self,tag_num,word,level):\n",
    "        self.tag_num=tag_num\n",
    "        self.word=word\n",
    "        self.emit_prob=None\n",
    "        self.parent=None\n",
    "        self.grandparent=None\n",
    "        self.grandparent_prob=0\n",
    "        self.grandparent_node=None\n",
    "        self.level=level\n",
    "        if self.word not in word_dict:\n",
    "            if self.tag_num!=tag_dict[most_used_tag]:\n",
    "                self.emit_prob=0\n",
    "            else:\n",
    "                self.emit_prob=1\n",
    "        else:\n",
    "            self.emit_prob=emit[word_dict[word]][self.tag_num]\n",
    "    \n",
    "    def result(self):\n",
    "        while self.grandparent:\n",
    "            #print(self.tag_num)\n",
    "            node.out.append(self.tag_num)\n",
    "            return self.grandparent.result()\n",
    "            \n",
    "class time_instance:\n",
    "    def __init__(self,word,level):\n",
    "        self.word=word\n",
    "        self.tag_count=tag_count\n",
    "        self.nodes=[]\n",
    "        \n",
    "        for i in range(self.tag_count):\n",
    "            #appending number of nodes equal to number of tags\n",
    "            self.nodes.append(node(i,word,level))\n",
    "        #print(f'timestep created with {len(self.nodes)} nodes')\n",
    "        '''self.nodes=[node for node in self.nodes1 if node.emit_prob!=0]'''\n",
    "        \n",
    "\n",
    "class timeline:\n",
    "    def __init__(self,seq):\n",
    "        self.seq=seq\n",
    "        self.tag_count=tag_count\n",
    "        self.timesteps=[]\n",
    "        #appending number of timesteps equal to number of words in input sequence\n",
    "        for i in range(len(self.seq)):\n",
    "            self.timesteps.append(time_instance(seq[i],i))\n",
    "        #print(f'timeline created with {len(self.timesteps)} timesteps')\n",
    "            \n",
    "    def parent_generator(self):\n",
    "        #this method is used if bigram model in cosidered for given sequence\n",
    "        #for first 2 words\n",
    "        #print('parent method called')\n",
    "        t1=self.timesteps[1]\n",
    "        ##bigram model is considerd for word at index 1\n",
    "        temp=0\n",
    "        first_tag=0\n",
    "        for node in (t1.nodes):\n",
    "            if node.emit_prob!=0:\n",
    "                #selecting node with maximum initial probability\n",
    "                if temp<trans[0][node.tag_num]*node.emit_prob:\n",
    "                    temp=trans[0][node.tag_num]*node.emit_prob\n",
    "                    first_tag=node\n",
    "        x=first_tag.tag_num\n",
    "        return x\n",
    "     #grand_parent_generator method is used for finding node with trigram assumption   \n",
    "    def grand_parent_generator(self):\n",
    "        #defining rult matrix\n",
    "        result=[0]\n",
    "        result.append(self.parent_generator())\n",
    "        #print('grand parent method called')\n",
    "        for time in range(2,len(self.timesteps)):\n",
    "            #print(f'{time}th timeline analysis')\n",
    "            #t2 is current timestep\n",
    "            t2=self.timesteps[time]\n",
    "            #time step before current\n",
    "            t1=self.timesteps[time-1]\n",
    "            #t0 is timestep before above timestep\n",
    "            t0=self.timesteps[time-2]\n",
    "            for n2 in range(len(t2.nodes)):\n",
    "                #print(f'{n2}th node')\n",
    "                #if for particular node emission probability is zero then that node is not considered as final probability will be also\n",
    "                if t2.nodes[n2].emit_prob!=0:\n",
    "                    temp_prob=0\n",
    "                    #tag_c is tag_num of node at current time \n",
    "                    tag_c=t2.nodes[n2].tag_num\n",
    "                    for n1 in range(len(t1.nodes)):\n",
    "                        #tag_b is tag_num at node at first previous timestep\n",
    "                        tag_b=t1.nodes[n1].tag_num\n",
    "                        #t_b_c is transition probability with tag_b\n",
    "                        t_b_c=trans[tag_b][tag_c]\n",
    "                        #if t_b_c is zero no path will be considered to reduce calculation\n",
    "                        if t_b_c!=0:\n",
    "                            for n0 in range(len(t0.nodes)):\n",
    "                                #tag_a is tag_num at second previous node\n",
    "                                tag_a=t0.nodes[n0].tag_num\n",
    "                                #t_b_c is transmission probability of current node with second previous node\n",
    "                                t_a_b=trans[tag_a][tag_b]\n",
    "                                if t_a_b*t2.nodes[n2].emit_prob>0:\n",
    "                                    #choosing node with max probability\n",
    "                                    if temp_prob<=t_a_b*t_b_c:\n",
    "                                        temp_prob=t_a_b*t_b_c\n",
    "                                        #updating grandparent object\n",
    "                                        t2.nodes[n2].grandparent=t1.nodes[n1]\n",
    "                                        #updating that probability at current node\n",
    "                                        t2.nodes[n2].grandparent_prob=temp_prob*t2.nodes[n2].emit_prob\n",
    "        #result evaluation    \n",
    "        #considering path with max probability\n",
    "        for j in range(len(self.timesteps)):\n",
    "            last_nodes=self.timesteps[j].nodes\n",
    "            result_prob=0\n",
    "            result_node=None\n",
    "            for i in range(len(last_nodes)):\n",
    "                if last_nodes[i].emit_prob!=0:\n",
    "                    if result_prob<=last_nodes[i].grandparent_prob:\n",
    "                        result_prob=last_nodes[i].grandparent_prob\n",
    "                        result_node=last_nodes[i]\n",
    "            result.append(result_node.tag_num)\n",
    "        x=result\n",
    "        #returning result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c63b1",
   "metadata": {},
   "source": [
    "Now using algorithm, each test input in given and accuracy is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53f4f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 85.92973061581336 %\n",
      "Sentense wise accuracy is: 7.2796934865900385%\n",
      "Total number of cases checked is:  783\n",
      "Number of failed cases is:  0\n",
      "number of failed cases in mismatch are:  0\n",
      "time required for execution is 55.359166622161865 seconds\n"
     ]
    }
   ],
   "source": [
    "#opening test sequence and corrosponding tag file\n",
    "test_seq=open('test_seq.txt','r')\n",
    "test_tag=open('test_tag.txt','r')\n",
    "test_seq.seek(0)\n",
    "test_tag.seek(0)\n",
    "out_tag=[]#all reulting tags\n",
    "expected_tag=[]#all expected tags\n",
    "failed_expected_tag=[]\n",
    "failed_cases_initial=0#if test case fails in algorithm then this variable is incremented\n",
    "failed_cases_mismatch=0##if test case with dimension then this variable is incremented    \n",
    "for test_case in range(test_nos):\n",
    "    #print(test_case)\n",
    "    a=test_seq.readline().split(' ')\n",
    "    b=[tag_dict[i] for i in test_tag.readline().split(' ')]\n",
    "    try:\n",
    "        test_timeline=timeline(a)\n",
    "        result=test_timeline.grand_parent_generator()#for each sentense grand parent_generator method is called\n",
    "        del result[2:4]\n",
    "        out_tag.append(result)\n",
    "        expected_tag.append(b)\n",
    "    except:\n",
    "        failed_cases_initial+=1\n",
    "        print(a)\n",
    "        print(f'failed at {test_case}')\n",
    "test_seq.close()\n",
    "test_tag.close()\n",
    "true=0\n",
    "false=0\n",
    "true_sentense=0\n",
    "false_sentense=0\n",
    "#calculating accuracy\n",
    "for i in range(len(out_tag)):\n",
    "    a=out_tag[i]\n",
    "    b=expected_tag[i]\n",
    "    if len(a)==len(b):\n",
    "        for j in range(len(out_tag[i])):\n",
    "            if a[j]==b[j]:\n",
    "                true+=1\n",
    "            else:\n",
    "                false+=1\n",
    "        if a==b:\n",
    "            true_sentense+=1\n",
    "        else:\n",
    "            false_sentense+=1\n",
    "    else:\n",
    "        failed_cases_mismatch+=1\n",
    "acc=true/(true+false)\n",
    "acc_sentense=true_sentense/(true_sentense+false_sentense)\n",
    "#printing result\n",
    "print(f'accuracy is: {acc*100} %')\n",
    "print(f'Sentense wise accuracy is: {acc_sentense*100}%' )\n",
    "later=time()\n",
    "print('Total number of cases checked is: ',test_nos)\n",
    "print('Number of failed cases is: ', failed_cases_initial)\n",
    "print('number of failed cases in mismatch are: ',failed_cases_mismatch)\n",
    "print(f'time required for execution is {later-now} seconds') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6516a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for <start> tag is 100.0\n",
      "accuracy for PRP tag is 99.67948717948718\n",
      "accuracy for VBP tag is 59.150326797385624\n",
      "accuracy for VBN tag is 74.32835820895522\n",
      "accuracy for CD tag is 99.64539007092199\n",
      "accuracy for NNS tag is 92.56880733944955\n",
      "accuracy for VBG tag is 93.63636363636364\n",
      "accuracy for JJ tag is 89.61578400830737\n",
      "accuracy for IN tag is 95.29072561268622\n",
      "accuracy for VBD tag is 80.89171974522293\n",
      "accuracy for NNP tag is 86.4406779661017\n",
      "accuracy for <end>\n",
      " tag is 100.0\n",
      "accuracy for DT tag is 99.23484402589759\n",
      "accuracy for NN tag is 62.83574380165289\n",
      "accuracy for VBZ tag is 92.87598944591029\n",
      "accuracy for RB tag is 96.17224880382776\n",
      "accuracy for JJR tag is 71.29629629629629\n",
      "accuracy for TO tag is 100.0\n",
      "accuracy for VB tag is 75.60975609756098\n",
      "accuracy for CC tag is 100.0\n",
      "accuracy for PRP$ tag is 93.6842105263158\n",
      "accuracy for MD tag is 98.96907216494846\n",
      "accuracy for NNPS tag is 56.25\n",
      "accuracy for RBS tag is 0.0\n",
      "accuracy for : tag is 96.66666666666667\n",
      "accuracy for WDT tag is 100.0\n",
      "accuracy for EX tag is 95.83333333333334\n",
      "accuracy for RP tag is 43.93939393939394\n",
      "accuracy for JJS tag is 79.3103448275862\n",
      "accuracy for WP tag is 97.91666666666666\n",
      "accuracy for RBR tag is 0\n",
      "accuracy for WRB tag is 100.0\n",
      "accuracy for WP$ tag is 100.0\n",
      "accuracy for PDT tag is 20.0\n",
      "accuracy for -LRB- tag is 100.0\n",
      "accuracy for -RRB- tag is 100.0\n",
      "accuracy for FW tag is 0\n",
      "accuracy for LS tag is 4.545454545454546\n",
      "accuracy for '' tag is 0\n",
      "accuracy for SYM tag is 0\n",
      "accuracy for # tag is 100.0\n",
      "accuracy for UH tag is 0\n",
      "accuracy for , tag is 0\n"
     ]
    }
   ],
   "source": [
    "#getting tag wose accuracy\n",
    "out_dict={i:0 for i in range(tag_count)}\n",
    "exp_dict={i:0 for i in range(tag_count)}\n",
    "for i in range(len(out_tag)):\n",
    "    for j in range(len(out_tag[i])):\n",
    "        exp_dict[out_tag[i][j]]+=1\n",
    "        if out_tag[i][j]==expected_tag[i][j]:\n",
    "            out_dict[out_tag[i][j]]+=1\n",
    "for i in range(tag_count):\n",
    "    if exp_dict[i]!=0:\n",
    "        out_dict[i]=out_dict[i]/exp_dict[i]*100\n",
    "        \n",
    "    else:\n",
    "        out_dict[i]=0\n",
    "def get_key(val):\n",
    "    for key, value in tag_dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "for i in range(len(out_dict)):\n",
    "    print(f'accuracy for {get_key(i)} tag is {out_dict[i]}' )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed3021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd32f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
